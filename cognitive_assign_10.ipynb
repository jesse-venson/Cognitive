{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6qlPGNclDhBA57/GNmufT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jesse-venson/Cognitive/blob/main/cognitive_assign_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "WTFWAqqh27AA",
        "outputId": "c46eb9d9-5feb-4fd0-96a0-812fcf56dfd8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.preprocessing.text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4de743f26269>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.preprocessing.text'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Q1: Text Preprocessing and Tokenization\n",
        "paragraph = \"\"\"Artificial Intelligence is transforming the world rapidly. Machines are now learning and adapting like humans.\n",
        "From healthcare to finance, AI is solving complex problems. It helps automate tasks and predict future outcomes.\n",
        "AI is also driving innovation in autonomous vehicles and robotics.\n",
        "The future of AI looks bright and limitless.\"\"\"\n",
        "\n",
        "# Lowercase and remove punctuation\n",
        "clean_text = re.sub(r'[^\\w\\s]', '', paragraph.lower())\n",
        "\n",
        "# Tokenize into words and sentences\n",
        "words_split = clean_text.split()\n",
        "words_nltk = word_tokenize(clean_text)\n",
        "sentences = sent_tokenize(clean_text)\n",
        "\n",
        "# Compare Python split() vs NLTK word_tokenize()\n",
        "print(\"Python split:\", words_split)\n",
        "print(\"NLTK word_tokenize:\", words_nltk)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words_nltk if word not in stop_words]\n",
        "\n",
        "# Frequency Distribution\n",
        "freq = Counter(filtered_words)\n",
        "print(\"Word Frequency (without stopwords):\", freq)\n",
        "\n",
        "# Q2: Extraction, Stemming, Lemmatization\n",
        "alpha_words = re.findall(r'\\b[a-zA-Z]+\\b', clean_text)\n",
        "filtered_alpha = [word for word in alpha_words if word not in stop_words]\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed = [stemmer.stem(word) for word in filtered_alpha]\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_alpha]\n",
        "\n",
        "print(\"\\nStemmed:\", stemmed)\n",
        "print(\"Lemmatized:\", lemmatized)\n",
        "print(\"Explanation: Stemming is quick but can produce non-words, lemmatization is grammar-aware and gives real words â€” preferable for clean NLP pipelines.\")\n",
        "\n",
        "# Q3: Feature Extraction with CountVectorizer and TF-IDF\n",
        "texts = [\n",
        "    \"AI is the future of technology.\",\n",
        "    \"Blockchain provides secure digital transactions.\",\n",
        "    \"Natural language processing enables machines to understand human language.\"\n",
        "]\n",
        "\n",
        "cv = CountVectorizer()\n",
        "bow = cv.fit_transform(texts)\n",
        "print(\"\\nBag of Words:\", bow.toarray())\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf.fit_transform(texts)\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "\n",
        "for i, text_vector in enumerate(tfidf_matrix):\n",
        "    scores = zip(feature_names, text_vector.toarray()[0])\n",
        "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "    top_keywords = [word for word, score in sorted_scores[:3]]\n",
        "    print(f\"Text {i+1} Top 3 Keywords:\", top_keywords)\n",
        "\n",
        "# Q4: Similarity Calculations\n",
        "text1 = \"\"\"Artificial Intelligence enables machines to learn and improve automatically based on data.\"\"\"\n",
        "text2 = \"\"\"Blockchain is a secure, decentralized system that records transactions across many computers.\"\"\"\n",
        "\n",
        "# Preprocess\n",
        "def preprocess(text):\n",
        "    return set(re.sub(r'[^\\w\\s]', '', text.lower()).split())\n",
        "\n",
        "set1, set2 = preprocess(text1), preprocess(text2)\n",
        "\n",
        "# Jaccard Similarity\n",
        "jaccard = len(set1.intersection(set2)) / len(set1.union(set2))\n",
        "print(\"\\nJaccard Similarity:\", jaccard)\n",
        "\n",
        "# Cosine Similarity\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([text1, text2])\n",
        "cos_sim = cosine_similarity(vectors[0:1], vectors[1:2])[0][0]\n",
        "print(\"Cosine Similarity:\", cos_sim)\n",
        "\n",
        "print(\"Insight: Jaccard works on overlap but ignores importance, Cosine uses frequency weight, usually better for text analysis.\")\n",
        "\n",
        "# Q5: Sentiment Analysis and Word Cloud\n",
        "review = \"The new smartphone is amazing with great features but the battery life is short.\"\n",
        "blob = TextBlob(review)\n",
        "print(\"\\nSentiment Polarity:\", blob.sentiment.polarity)\n",
        "print(\"Subjectivity:\", blob.sentiment.subjectivity)\n",
        "\n",
        "# Classification\n",
        "if blob.sentiment.polarity > 0:\n",
        "    sentiment = \"Positive\"\n",
        "elif blob.sentiment.polarity < 0:\n",
        "    sentiment = \"Negative\"\n",
        "else:\n",
        "    sentiment = \"Neutral\"\n",
        "print(\"Review Sentiment:\", sentiment)\n",
        "\n",
        "# Word Cloud\n",
        "positive_reviews = \"This laptop is excellent. Great performance and fantastic battery life.\"\n",
        "wordcloud = WordCloud(background_color='white').generate(positive_reviews)\n",
        "\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Q6: Text Generation with Keras\n",
        "train_text = \"\"\"Artificial intelligence enables machines to learn from data, improving their performance with experience. AI applications include image recognition, natural language processing, and autonomous vehicles.\"\"\"\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([train_text])\n",
        "sequences = []\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "token_list = tokenizer.texts_to_sequences([train_text])[0]\n",
        "\n",
        "for i in range(1, len(token_list)):\n",
        "    sequences.append(token_list[:i+1])\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = np.eye(total_words)[y]\n",
        "\n",
        "# Simple Model\n",
        "model = Sequential([\n",
        "    Embedding(total_words, 10, input_length=X.shape[1]),\n",
        "    LSTM(50),\n",
        "    Dense(total_words, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=500, verbose=0)\n",
        "\n",
        "# Generate Text\n",
        "seed_text = \"Artificial intelligence\"\n",
        "next_words = 3\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    token_list = np.pad(token_list, (X.shape[1] - len(token_list), 0), 'constant')\n",
        "    predicted = np.argmax(model.predict(token_list.reshape(1, -1), verbose=0), axis=-1)\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            seed_text += \" \" + word\n",
        "            break\n",
        "\n",
        "print(\"\\nGenerated Text:\", seed_text)\n",
        "\n"
      ]
    }
  ]
}